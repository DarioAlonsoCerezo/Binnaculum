---
name: Flaky Test Detection

on:
  schedule:
    # Run weekly on Sundays at 3 AM UTC
    - cron: '0 3 * * 0'
  workflow_dispatch:
    inputs:
      test_runs:
        description: 'Number of test runs for flaky detection'
        required: true
        default: '10'
        type: string
      test_filter:
        description: 'Test filter (optional)'
        required: false
        default: ''
        type: string

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  DOTNET_NOLOGO: true
  DOTNET_SKIP_FIRST_TIME_EXPERIENCE: true
  DOTNET_CLI_TELEMETRY_OPTOUT: true

jobs:
  # Run tests multiple times to detect flakiness
  flaky-test-detection:
    name: Flaky Test Detection (Run ${{ matrix.run }})
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        run: ${{ fromJson(format('[{0}]', join(range(1, fromJson(github.event.inputs.test_runs || '10') + 1), ','))) }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup .NET 9
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: '9.0.x'

      - name: Restore dependencies
        run: dotnet restore src/Tests/Core.Tests/Core.Tests.fsproj

      - name: Build Core project
        run: dotnet build src/Core/Core.fsproj --configuration Release --no-restore

      - name: Build Core.Tests
        run: dotnet build src/Tests/Core.Tests/Core.Tests.fsproj --configuration Release --no-restore

      - name: Run Core.Tests (Run ${{ matrix.run }})
        run: |
          TEST_FILTER="${{ github.event.inputs.test_filter }}"
          if [ -n "$TEST_FILTER" ]; then
            FILTER_ARGS="--filter \"$TEST_FILTER\""
          else
            FILTER_ARGS=""
          fi
          
          dotnet test src/Tests/Core.Tests/Core.Tests.fsproj \
            --configuration Release --no-build --verbosity normal \
            $FILTER_ARGS \
            --logger "trx;LogFileName=run-${{ matrix.run }}-results.trx" \
            --collect:"XPlat Code Coverage"
        continue-on-error: true

      - name: Upload test results (Run ${{ matrix.run }})
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: flaky-test-results-run-${{ matrix.run }}
          path: |
            TestResults/run-${{ matrix.run }}-results.*
            **/*coverage*.xml
          retention-days: 7

  # Analyze results for flakiness
  analyze-flakiness:
    name: Analyze Test Flakiness
    needs: [flaky-test-detection]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python for analysis
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Download all test results
        uses: actions/download-artifact@v4
        with:
          pattern: 'flaky-test-results-run-*'
          merge-multiple: true

      - name: Install analysis dependencies
        run: |
          pip install lxml beautifulsoup4 pandas

      - name: Analyze test flakiness
        run: |
          cat << 'EOF' > analyze_flakiness.py
          import xml.etree.ElementTree as ET
          import glob
          import json
          from collections import defaultdict, Counter
          import pandas as pd
          
          def analyze_test_results():
              test_results = defaultdict(list)  # test_name -> [pass/fail/skip]
              total_runs = 0
              
              # Parse all JUnit XML files
              for xml_file in glob.glob("**/run-*-results.trx", recursive=True):
                  try:
                      tree = ET.parse(xml_file)
                      root = tree.getroot()
                      total_runs += 1
                      
                      # Handle different JUnit XML formats
                      testcases = root.findall('.//testcase')
                      if not testcases:
                          testsuites = root.findall('.//testsuite')
                          for testsuite in testsuites:
                              testcases.extend(testsuite.findall('testcase'))
                      
                      for testcase in testcases:
                          name = testcase.get('name', 'Unknown')
                          classname = testcase.get('classname', 'Unknown')
                          full_name = f"{classname}.{name}"
                          
                          # Determine test result
                          if testcase.find('failure') is not None:
                              result = 'fail'
                          elif testcase.find('error') is not None:
                              result = 'error'
                          elif testcase.find('skipped') is not None:
                              result = 'skip'
                          else:
                              result = 'pass'
                          
                          test_results[full_name].append(result)
                  except Exception as e:
                      print(f"Error parsing {xml_file}: {e}")
              
              return test_results, total_runs
          
          def calculate_flakiness(test_results, total_runs):
              flaky_tests = []
              
              for test_name, results in test_results.items():
                  if len(results) < total_runs:
                      # Test didn't run in all attempts - potentially flaky
                      missing_runs = total_runs - len(results)
                      results.extend(['missing'] * missing_runs)
                  
                  counter = Counter(results)
                  pass_count = counter['pass']
                  fail_count = counter['fail'] + counter['error']
                  skip_count = counter['skip']
                  missing_count = counter['missing']
                  
                  # Calculate flakiness metrics
                  if fail_count > 0 and pass_count > 0:
                      flakiness_rate = fail_count / (pass_count + fail_count)
                      flaky_tests.append({
                          'test_name': test_name,
                          'pass_count': pass_count,
                          'fail_count': fail_count,
                          'skip_count': skip_count,
                          'missing_count': missing_count,
                          'flakiness_rate': flakiness_rate,
                          'total_runs': len(results),
                          'results': results
                      })
              
              return sorted(flaky_tests, key=lambda x: x['flakiness_rate'], reverse=True)
          
          def generate_report(flaky_tests, total_runs):
              report = {
                  'total_runs': total_runs,
                  'total_flaky_tests': len(flaky_tests),
                  'flaky_tests': flaky_tests[:10],  # Top 10 most flaky
                  'summary': {
                      'high_flakiness': len([t for t in flaky_tests if t['flakiness_rate'] > 0.3]),
                      'medium_flakiness': len([t for t in flaky_tests if 0.1 < t['flakiness_rate'] <= 0.3]),
                      'low_flakiness': len([t for t in flaky_tests if 0.0 < t['flakiness_rate'] <= 0.1])
                  }
              }
              
              with open('flakiness_report.json', 'w') as f:
                  json.dump(report, f, indent=2)
              
              return report
          
          if __name__ == "__main__":
              test_results, total_runs = analyze_test_results()
              print(f"Analyzed {total_runs} test runs")
              print(f"Found {len(test_results)} unique tests")
              
              flaky_tests = calculate_flakiness(test_results, total_runs)
              report = generate_report(flaky_tests, total_runs)
              
              print(f"Found {len(flaky_tests)} flaky tests")
              for test in flaky_tests[:5]:
                  print(f"- {test['test_name']}: {test['flakiness_rate']:.2%} flaky ({test['fail_count']}/{test['total_runs']} failures)")
          EOF
          
          python analyze_flakiness.py

      - name: Generate flakiness report
        run: |
          if [ -f flakiness_report.json ]; then
            echo "## ðŸŽ­ Flaky Test Detection Report" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Analysis Date**: $(date -u +'%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
            echo "**Total Test Runs**: $(jq -r '.total_runs' flakiness_report.json)" >> $GITHUB_STEP_SUMMARY
            echo "**Total Flaky Tests Found**: $(jq -r '.total_flaky_tests' flakiness_report.json)" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            HIGH=$(jq -r '.summary.high_flakiness' flakiness_report.json)
            MEDIUM=$(jq -r '.summary.medium_flakiness' flakiness_report.json)
            LOW=$(jq -r '.summary.low_flakiness' flakiness_report.json)
            
            echo "### Flakiness Breakdown:" >> $GITHUB_STEP_SUMMARY
            echo "- ðŸ”´ High flakiness (>30%): $HIGH tests" >> $GITHUB_STEP_SUMMARY
            echo "- ðŸŸ¡ Medium flakiness (10-30%): $MEDIUM tests" >> $GITHUB_STEP_SUMMARY
            echo "- ðŸŸ¢ Low flakiness (<10%): $LOW tests" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            if [ "$HIGH" -gt 0 ] || [ "$MEDIUM" -gt 0 ]; then
              echo "### Top Flaky Tests:" >> $GITHUB_STEP_SUMMARY
              echo '```json' >> $GITHUB_STEP_SUMMARY
              jq -r '.flaky_tests[:5] | .[] | "\(.test_name): \(.flakiness_rate * 100 | round)% flaky (\(.fail_count)/\(.total_runs) failures)"' flakiness_report.json >> $GITHUB_STEP_SUMMARY
              echo '```' >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "âš ï¸ **Action Required**: Please investigate and fix flaky tests to improve CI reliability." >> $GITHUB_STEP_SUMMARY
            else
              echo "âœ… **No significant flaky tests detected!**" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "âŒ No flakiness report generated - check test execution logs" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload flakiness analysis
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: flakiness-analysis-${{ github.run_number }}
          path: |
            flakiness_report.json
            analyze_flakiness.py
          retention-days: 30

      - name: Create issue for high flakiness
        if: always()
        run: |
          if [ -f flakiness_report.json ]; then
            HIGH=$(jq -r '.summary.high_flakiness' flakiness_report.json)
            MEDIUM=$(jq -r '.summary.medium_flakiness' flakiness_report.json)
            
            if [ "$HIGH" -gt 0 ] || [ "$MEDIUM" -gt 5 ]; then
              echo "ðŸš¨ High flakiness detected!"
              echo "High flakiness tests: $HIGH"
              echo "Medium flakiness tests: $MEDIUM"
              echo ""
              echo "Top flaky tests:"
              jq -r '.flaky_tests[:5] | .[] | "- \(.test_name): \(.flakiness_rate * 100 | round)% flaky"' flakiness_report.json
              echo ""
              echo "Consider creating an issue to track flaky test improvements."
              # Note: In a real implementation, you might use GitHub API to create an issue
            fi
          fi